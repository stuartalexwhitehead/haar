\chapter{Implementation}

  \section{Development Process}
    A software implementation can be set up for success or failure before any code has been written, especially whilst working within a team. This is down to the development process and policies in place. Of course, one process may work for one team or project and not another.

    A well implemented process can provide many benefits. First and foremost, everyone in a development team clearly understands their roles and responsibilities. The process can also enforce a hierarchy of decision making, or encourage peer collaboration. Beyond these people-based benefits, a development process can aid in the development of a robust, maintainable codebase. A developer who can understand a codebase and make confident updates is an efficient one.

    Although the output of this project is the work of one person, arguments supporting a process-first implementation still apply. The following section will detail the main tools used in supporting this project. 

    \subsection{Version Control System}
      The use of a Version Control System (VCS) is highly recommended in any project. As the name suggests, a VCS manages iterative updates of project files. To generalise its functionality, a VCS maintains a timeline of file states. Different states of the same file can be compared to investigate which changes were made and by whom, and if needed a file can be reverted to a previous state. This means that the complete history of a codebase is recorded. Without using a VCS, code updates would simply overwrite the existing file and old states would be lost forever.

      There are a number of Version Control Systems available to developers, such as Mercurial and Subversion. Git is another VCS and was created by Linus Torvalds, creator and maintainer of the Linux kernel. Git is arguably the most popular VCS amongst the open source community, thanks to its use in the development of the Linux kernel. Its command-line interface is highly portable and many community-oriented services support it extensively, such as Github and Bitbucket.

      Git along with the web-based service GitHub have been chosen for use in this project thanks to their universal support. But simply selecting a tool is not enough when implementing a development process - guidelines on how to use it should also be decided. There are a number of well-defined development workflows using Git. This project has chosen to implement the `Git Flow' workflow. Git Flow utilises various features of Git and GitHub, including branching, merging and pull requests. 

      Describing the Git and the Git Flow workflow could fill an article on its own, so this description will be very brief. In Git, a branch is an instance of the complete codebase. Multiple branches with different versions of the same codebase can be developed in parallel. The merge command copies the changes of one branch into another. A Pull Request is a request to merge one branch into another, which can be inspected and commented on before the merge is completed.

      The Git Flow workflow adds structure to these features. The master branch is the production codebase. The develop branch is the in-progress codebase. Feature branches (such as feature/menu or feature/users-controller) are used to develop a specific feature of the project. Once the feature has been built, a pull request is raised to merge it into the develop branch. Once all features have been merged and tested, a pull request is raised to merge the develop branch into master which is then deployed. This workflow has been employed for this project.

      \subsubsection{Pre-commit Hook}
        The Git VCS also supports lifecycle hooks. These hooks allow developers to run bespoke scripts to enforce custom policies, such as limiting access to specific branches. One helpful trick has also been employed for this project through the use of the pre-commit hook.

        No matter how descriptive or strict development policies are, developers are only human. Humans can make mistakes and they can be lazy, meaning that poor quality code can be committed to the codebase. The pre-commit hook can be used to enfoce a specific coding standard. It works by testing the committed code against a set of rules. If the tests pass, the code is committed to the codebase but if they fail, a warning message is displayed. By using this technique, the code for this project is guaranteed to be of a certain standard.

    \subsection{Docker}
      Developers face a number of challenges when developing and deploying modern web-based applications. Primarily, the environment in which code is executed can differ wildly. For any given project, one developer might use a Windows machine whilst their colleague could use an Apple Mac. The staging environment could use one version of Linux whereas the production enviroment could use another. All of these environments could potentially run differing versions of the same software meaning that bugs in the project might not be traceable or reproducable. Development teams have attempted to normalise these enviroments through the use of package managers, however these still leave room for misconfiguration.

      A relatively new tool called Docker has been taking the development world by storm since its release in 2013. Docker builds on a technology called Linux Containers. Traditional virtual machines run a complete operating system (kernel, program binaries and any other files) on top of a host machine. In comparison, Docker (and Linux Containers) allow images to use the same kernel of the host operating system. This means that each container is very lightweight but can still benefit from the complete isolation of program processes.

      Docker containers are instances of a Docker image. Docker images are compiled according to a Dockerfile which comprise of commands such as COPY (to copy source files to the image) and RUN (to run Unix commands within the Linux Container). This is where the advantages of Docker become clear - explicit, reproducable environments can be built as if working directly on a server. Since Docker is a portable tool, every developer working on a project can run exactly the same environment as each other. When the application is ready to be deployed, the same Docker image can be executed on a production server.

      Docker also has powerful composition tools. Anything beyond a simple script or application is typically the sum of many smaller, more specific applications. A simple example of this would be a dynamic web application backed by a database. In Docker terms, this application would be composed of two images - one image for the dynamic web application, and another for the database. Docker can configure automatic links between instances of these images and this is aided with the helped command-line interface `docker compose'.

      Docker Compose makes use of another configuration file, `docker-compose.yml'. In this file it is possible to specify which Docker images make up a complete application (which can be custom builds using a Dockerfile, or pre-built images from the global Docker repository). Using this file Docker Compose will intelligently start Docker images in order of their dependencies. Using the dynamic web application and database example, the database would be started first and then the application. Docker will also set up lots of other clever tools, such as a long list of environment variables referring to specific links. These environment variables can then be used within applications to configure things like database connections.

      Once applications have been wrapped up into Docker images and compositions, it is then necessary to deploy them to servers. This can of course be achieved manually using the docker CLI, however the Docker ecosystem also includes Docker Cloud, a first-party container deployment service. Docker Cloud uses four more important terms to describe an application:

      \begin{description}
        \item[Node] a server which can host Docker containers
        \item[Node Cluster] a grouping of multiple servers which can host Docker containers
        \item[Service] one or more running container instances of a Docker image
        \item[Stack] an application composed of multiple services
      \end{description}

      Docker Cloud is very simple but hugely powerful. Scaling is an important aspect of any web application and is handled with grace. Docker Cloud will intelligently distribute container instances across all available nodes based on their loads. It will then lay a private network between all services of a stack (irrespective of its host node) so all containers can transparently communicate. This makes load balancing truly simple.

      Docker has been chosen as a critical piece of infrastructure for this project. Flexibility, robustness and maintainability are three important aspects when developing any application and Docker aids these areas very effectively.

    \subsection{Continuous Integration}
      The development process has so far defined how the codebase will be developed and how it will be hosted, but there is still an important step missing. How does newly developed code get deployed in a way which ensures no downtime and without bugs? This is where a technique called Continuous Integration (CI) is used.

      Continuous Integration is the glue between the processes at each end of the development cycle. The main ethos behind it is to deploy little and often. This allows developers to catch issues early and to pin-point its cause quickly. \citet{continuous-integration} describe CI with a number of key points, including: automated, self-testing builds and automated deployments. As it turns out, the Git Flow workflow, Docker and Continuous Integration all work incredibly well together.

      As has already been discussed, Docker images provide explicit, reproducable environments and this is great for testing purposes. Since all Docker environments will be built equal, it means tests run on a local machine will behave the same on a production server. And because Docker Compose can specify which images needs to be used, any service with access to this information can build the correct application dependencies.

      Travis CI is one such continuous integration service which supports the Docker ecosystem. It has been employed in this project to enforce a CI workflow and it follows these steps:

      \begin{enumerate}
        \item Code is developed using the Git Flow workflow
        \item All commits to branches and pull requests trigger tests on Travis CI
        \item Each test builds the Docker environment to ensure consistency
        \item If tests pass, the Docker image is published to the Docker repository
        \item The Docker repository will deploy new images through Docker Cloud
      \end{enumerate}

      This workflow contributes to a solid development process which can be scaled to a large codebase and team of developers.

  \section{Haar Engine}
    Chapter \ref{Chapter:Specification} (Specification) states that this project will create two main software components: Haar Engine and Haar. This section will discuss the implementation of the former - the generic IoT application framework.

    The most important decision that was made for this component was the programming environment used for its development. The design requirements for the framework could be mostly satisfied by a number of web-oriented technologies, such as PHP, Python or Ruby. One of the main features of the Haar Engine is the ability to handle bi-directional, real-time communication channels. As was mentioned in the Design chapter, one programming environment stood out for this requirement - Node.js.

    Node.js is a server-side implementation of JavaScript. More specifically it implements Google's V8 JavaScript engine and it is ideal for this project due to its event-driven nature. In general, it is based on the event loop concurrency model - a single-threaded process which loops continuously over a message queue \citep{event-loop}. Haar Engine is expected to be focussed on input and output (IO) operations. IO operations using more traditional techologies such as PHP, Python and Ruby are synchronous in nature and block futher execution of the process. In comparison, JavaScript (and therefore Node.js) can continue processing other instructions while waiting for an IO operation to complete.

    JavaScript itself is an implementation of the ECMAScript standard. ECMAScript has seen a number of updates and is currently on a version dubbed ECMAScript 2015 \citep{es2015}. This latest version adds a long list of new syntax and tools to the langauge. Client-side JavaScript has historically suffered from poor universal compatibility due to differences in browser implementations, however since Node.js is a server-side tool and under a developer's control, this project will be implemented using native ES2015 syntax.

    Another important decision made was the database support offered by this framework. Originally, section \ref{section:database-connector} (Database Connector) specified that the framework should offer a Database Conector concept where a developer can implement a database of their own choosing. Whilst investigating possible database types, it was decided that this options was simply too broad giving the time constraints for this project. Rather, Haar Engine will support one, highly extensible database solution. The NoSQL databse MongoDB was selected for this purpose.

    MongoDB was selected for a number of reasons. First of all, the flexibility of a NoSQL versus a relational SQL database allows the schema to be flexible and to change with the data. This makes it simpler for a developer to add new data structures. In comparison, to modify an SQL database the developer would have to first update the database schema using a Data Definition Language (DDL). MongoDB was selected from the avaiable NoSQL database solutions because its data structure is based on JavaScript Object Notation (JSON). This makes it easy to interface with the Node.js application code. There are also a number of helper packages available for Node.js.

    Once the fundamental technologies were chosen, the next step was to go ahead and build the framework. Since the data models and data access API are used to configure the fundamental aspects of the system, they were developed first. The structure of Haar Engine is based on the Model View Controller (MVC) architecture. The models were defined using the Mongoose package, a tool which helps to give structure to the otherwise freeform MongoDB database. Controllers were then built using the popular Express package. They are responsible for authentication, data handling and response building. Controllers will respond with JSON objects which are considered the views of this system.

    One important aspect of the data access API is that of security. The API should only allow authenticated and authorised users to perform actions, so a robust mechanism to handle this functionality was required. There are two generalised methods which can be employed to achieve this - stateful and stateless session management. With stateful sessions, the application server is required to store information about the user. The session data will then be looked up upon each request. In comparison, stateless sessions do not require the server to store session data because all information is sent along each request. Stateless sessions are more transparent and they scale easier, so they have been chosen for Haar Engine.

    User authentication is enforced through the use of stateless sessions and JSON Web Tokens (JWT). The first step in this process is to authenticate a user through a username and password combination. If the user credentials are valid, a JWT is generated and signed using the user's profile data and a secret key. This token is then returned as the response to an authentication request. Any further requests to the Data Access API should include this token. Requests to any API resource will be verified server-side to ensure a JWT is available and is valid.

    Once the fundamental software components were in place, it was then time to build the \textit{raison d'etre} of this project - real-time, bi-directional communication. As has been discussed, MQTT would be an ideal candidate for this project if it were not for its reliance on uncommon TCP ports. Instead, Haar Engine has used the publish-subscribe concept of MQTT as inspiration for a real-time implementation using WebSockets.

    The specific implementation of this feature had to be revised from the initial plan in Chapter \ref{chapter:design} (Design). In that chapter, the JavaScript package Socket.io was identified as a suitable tool. However during development of the Real-Time API, limitations in Socket.io meant that not all features of the publish-subscribe concept could be effectively implemented. An alternative JavaScript package called Primus was used as the basis for this feature instead. Using Primus, Haar Engine successfully allows clients to publish device data and to subscribe to data events. As a bonus, a feature of Primus allows custom `middleware' to be written in order to authenticate connections. This allowed Haar Engine to reuse existing the existing JWT authentication mechanism.

    As should be done with any modestly-sized software application, a suite of unit tests was developed for Haar Engine. The unit tests developed embraced the stengths of Node.js and Docker. Since the Docker environment ensures that all dependencies are started, it means that the unit tests can use a live database rather than mocked data. When testing the models, the database was cleared and seeded with data before each test to ensure testing consistency. When testing the APIs, HTTP requests are made to the application in order to replicate real-world scenarios.

    One final point worthy of discussion is the evaluation and execution of device rules. As a reminder, rules operate on sensor data and trigger an actuator. To provide as much flexibility as possible, each rule is a complete JavaScript script with access to variables and system functions. This, however, presents its own challenges. Accepting user input - especially code- is dangerous and could compromise the integrity of the host application, and this scenerio is no different. To protect from this risk, rules are executed in a sandbox environment with access to only their required variables. This prevents user-submitted scripts from tampering or otherwise affecting normal execution of the framework. An additional benefit to using this sandbox technique is the ability to catch syntax errors. When storing a new rule, it can be evaluated with example data and any resulting syntax errors can be returned to the user as validation.

  \section{Haar: API}
    The second software component produced for this project is Haar, a demonstration implementation of Haar Engine. The main purpose of this demonstration application is to validate the design decisions and to show, by example, that the theory behind this IoT framework works. The implementation of Haar has been split further into four sub-components: API, Dashboard, Bridge and Nodes.

    Haar API is the actual implementation of Haar Engine. Since the aim of Haar Engine is to provide a starting point for an IoT application, the sole purpose of Haar API is to bootstrap the framework and show that it can be included as a project dependency. Quite simply, Haar API includes Haar Engine as a dependency from Github.

  \section{Haar: Dashboard}
    Haar Dashboard is a web-based user interface built on top of the suite of Haar Engine APIs. It utilises both the Data Access API to configure system settings and the Real-Time API to subscribe to live data events. Whilst not necessary for the operation of the underlying IoT framework, the Haar Dashboard UI makes configuring devices simple and illustrates how a viable consumer solution might work.

    Section \ref{section:front-end-application} (Front-End Application) briefly discussed the challenges facing this user interface. In particular, certain components of the user interface must react to real-time events. To address this challenge, the UI has been developed as a so-called `single-page JavaScript web app'. JavaScript web apps come with their own challenges. Web pages using this technique are typically not built on the application server but are rather 100\% client-side code. Due to this, single-page web apps can experience a long initial delay as the client-side script is executed. Another issues is that search engine optimisation and accessibility are affected because no HTML body is returned by the server - there is nothing for search engine crawlers or accessibility tools to parse.

    A clever JavaScript framework called React solves these issues. React is a project developed by software engineers at Facebook. It is based on the concept of a tree composable components, where a single component encapsulates all of the logic which it requires to execute. What is most clever about React is its use of a virtual Document Object Model (DOM). React will first build up the tree of components using this virtual DOM before `diffing' it against the actual DOM. If anything has changed, React will only update the affected components. Using this technique, React can be used on a web application server. It can build up a webpage and send it as the body of an HTML response. Once the client-side code has loaded, React will then bootstrap the application based on the existing body. This concept is known as Isomorphic or Universal JavaScript.

    As was discussed for the implementation of Haar Engine, the latest features of JavaScript are defined in the ECMAScript 2015 standard. Some web browsers can be slow to implement new features whilst other, older browsers may not implement them at all. This is a headache for developers; new features and syntax might solve problems in a more effective way, but they may not be supported by every target browser. This is where so-called front-end tooling is used. A variety of tools exist to convert new, uncompatible syntax into older, widely supported syntax. In particular a package called Babel has been used in this project to compile ES2015 syntax into ECMAScript 5 compatible code, meaning it can be run in all modern browsers. Additionally, a tool called Webpack has been used to package server-side JavaScript modules for use within the browser.

    The features which React provide solve half of the front-end engineering problem. React defines how data is manipulated and displayed, but it does not specify how the data itself is managed. For that reason, Facebook have also invented a development pattern called Flux. Flux provides a strict method by which data is added or updated then introduced to the React component tree. The fundamental rule of Flux is that the data flow is unidirectional; that is, data cannot be mutated directly but rather through a pre-defined process. Figure x.x illustrates this process. Redux has become the \textit{de facto} standard of this pattern and will be used in this project.

    \begin{figure}
    \centering
      \begin{tikzpicture}
        \draw (0,0) rectangle (2.5, 1.25);
        \node[align=center] at (1.25,0.625) {Action};
        \draw[->, >=stealth] (2.5,0.625) -- (3.5,0.625);

        \draw[<-, >=stealth] (4.75,1.25) -- (4.75,2.875) -- (7,2.875);
        \draw (7,2.25) rectangle (9.5, 3.5);
        \node[align=center] at (8.25,2.875) {Action};
        \draw[<-, >=stealth] (9.5,2.875) -- (11.75,2.875) -- (11.75,1.25);

        \draw (3.5,0) rectangle (6, 1.25);
        \node[align=center] at (4.75,0.625) {Dispatcher};
        \draw[->, >=stealth] (6,0.625) -- (7,0.625);

        \draw (7,0) rectangle (9.5, 1.25);
        \node[align=center] at (8.25,0.625) {Store};
        \draw[->, >=stealth] (9.5,0.625) -- (10.5,0.625);

        \draw (10.5,0) rectangle (13, 1.25);
        \node[align=center] at (11.75,0.625) {View};
      \end{tikzpicture}
    \caption{Data flow in a Flux React application}\label{figure:flux-flow}
  \end{figure}

  The Flux development pattern is also very effective at managing real-time data events. The Primus package used in Haar Engine also has a client-side library builder and this has been used in Haar Dashboard to manage the WebSocket connection. Haar Dashboard will subscribe to data streams managed by Haar API and when data is received, a Redux action will be dispatched and handled appropriately.

  \section{Haar: Nodes}
  \section{Haar: Bridge}
